{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Install Required Libraries\n",
    "- !pip install psycopg2-binary\n",
    "- !pip install sentence-transformers\n",
    "- !pip install openai  # Only if you plan to use OpenAI's API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Connect to PostgreSQL and Retrieve Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ancha\\AppData\\Local\\Temp\\ipykernel_38076\\1466140673.py:27: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  recipes_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    name  \\\n",
      "0  Peanut Butter Banana Protein Smoothie   \n",
      "1  Peanut Butter Banana Protein Smoothie   \n",
      "2  Peanut Butter Banana Protein Smoothie   \n",
      "3  Peanut Butter Banana Protein Smoothie   \n",
      "4  Peanut Butter Banana Protein Smoothie   \n",
      "\n",
      "                                         description  calories  total_fat  \\\n",
      "0  A nutritious and satisfying beverage for break...       482       8.34   \n",
      "1  A nutritious and satisfying beverage for break...       482       8.34   \n",
      "2  A nutritious and satisfying beverage for break...       482       8.34   \n",
      "3  A nutritious and satisfying beverage for break...       482       8.34   \n",
      "4  A nutritious and satisfying beverage for break...       482       8.34   \n",
      "\n",
      "   protein  carbs              ingredient_text  \\\n",
      "0    28.57  77.97                        Honey   \n",
      "1    28.57  77.97                        Honey   \n",
      "2    28.57  77.97                        Honey   \n",
      "3    28.57  77.97  Oikos Triple Zero - Vanilla   \n",
      "4    28.57  77.97  Oikos Triple Zero - Vanilla   \n",
      "\n",
      "                      instruction_text  \n",
      "0  Transfer to a tall glass and serve.  \n",
      "1     Add ice for desired consistency.  \n",
      "2      Mix all ingredients in blender.  \n",
      "3  Transfer to a tall glass and serve.  \n",
      "4     Add ice for desired consistency.  \n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Database connection details\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'recipe_db',\n",
    "    'user': 'postgres',\n",
    "    'password': '',\n",
    "    'port': '5433'  \n",
    "}\n",
    "\n",
    "# Establish connection to PostgreSQL\n",
    "conn = psycopg2.connect(**db_config)\n",
    "\n",
    "# Retrieve recipes and ingredients data\n",
    "query = \"\"\"\n",
    "SELECT r.name, r.description, n.calories, n.total_fat, n.protein, n.carbs, \n",
    "       i.ingredient_text, d.instruction_text\n",
    "FROM recipes r\n",
    "JOIN nutrition_facts n ON r.recipe_id = n.recipe_id\n",
    "JOIN ingredients i ON r.recipe_id = i.recipe_id\n",
    "JOIN directions d ON r.recipe_id = d.recipe_id;\n",
    "\"\"\"\n",
    "\n",
    "# Load data into a DataFrame\n",
    "recipes_df = pd.read_sql(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# Preview the retrieved data\n",
    "print(recipes_df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The code successfully retrieved data from the PostgreSQL database, showing a sample output with five rows. Each row includes:\n",
    "\n",
    "    - name: Name of the recipe\n",
    "    - description: Description of the recipe\n",
    "    - calories, total_fat, protein, carbs: Nutritional information\n",
    "    - ingredient_text: Ingredients for the recipe\n",
    "    - instruction_text: Step-by-step instructions\n",
    "- The warning about using SQLAlchemy can be ignored here, as the query still succeeded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Data Preprocessing\n",
    "- Combine the recipe description, ingredients, and directions into a single text field for each recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine fields into a single text field for each recipe\n",
    "recipes_df['text'] = recipes_df['description'] + \" \" + recipes_df['ingredient_text'] + \" \" + recipes_df['instruction_text']\n",
    "\n",
    "# Group by recipe name to aggregate all ingredients and instructions per recipe\n",
    "recipes_texts = recipes_df.groupby('name')['text'].apply(' '.join).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Generate Embeddings\n",
    "- Use a pre-trained model like Sentence Transformers to generate embeddings for each recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df39751a13041c2a1525e19ef620500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ancha\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ancha\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b4b04a5db24e27b3981a9ff1d73050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832c53c55f6e413fa81070b51e88bde0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1792d88d56f24b7cbe7e09bcb9c10c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6a20181b264006bb12847f45c221e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3089a5be6613471baacdf32859cf7fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb049a8914049778fcb7ace3adc68b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896c74f354504533ae6689746db6ba71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1411495fa34704bb6147bd9bfc9af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e41ef6b41e443982385423ca769ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588747dae09d48a2aaf02a769d530a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # You can choose other models if preferred\n",
    "\n",
    "# Generate embeddings for each recipe text\n",
    "recipes_texts['embedding'] = recipes_texts['text'].apply(lambda x: model.encode(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The model, sentence-transformers/all-MiniLM-L6-v2, was successfully loaded, but with a warning related to caching limitations on Windows. This warning indicates that caching files might use more disk space if symbolic links are not supported, which is generally a minor concern unless disk space is limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5 (Optional): Save Embeddings to Database\n",
    "- If you want to save these embeddings back to the database for faster retrieval, you'll need to serialize them. Here’s an example using PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "566"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Establish connection using SQLAlchemy for easier writing back\n",
    "engine = create_engine(f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config.get('port', 5433)}/{db_config['database']}\")\n",
    "\n",
    "# Serialize embeddings as binary data\n",
    "recipes_texts['embedding'] = recipes_texts['embedding'].apply(lambda x: np.array(x).tobytes())\n",
    "\n",
    "# Save to a new table (or update an existing one)\n",
    "recipes_texts[['name', 'embedding']].to_sql('recipe_embeddings', engine, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output 566 indicates that embeddings were generated for 566 recipes in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Build a Simple Recommendation Function\n",
    "- Calculate similarity scores between the user’s input and each recipe based on the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           name  similarity\n",
      "1626  Ham Cheddar and Jalapeno Crustless Quiche    0.668146\n",
      "1132                       Cream Cheese Biscuit    0.627648\n",
      "3343                   Vanilla Protein Pancakes    0.613065\n",
      "299                 Banana Stuffed French Toast    0.611616\n",
      "2982         Strawberry Banana Protein Smoothie    0.598489\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def recommend_recipes(user_input, recipes_texts, model, top_n=5):\n",
    "    # Generate embedding for user input\n",
    "    user_embedding = model.encode(user_input)\n",
    "\n",
    "    # Calculate similarity scores\n",
    "    recipes_texts['similarity'] = recipes_texts['embedding'].apply(\n",
    "        lambda x: cosine_similarity([user_embedding], [np.frombuffer(x, dtype=np.float32)]).flatten()[0]\n",
    "    )\n",
    "\n",
    "    # Sort by similarity and return top N recommendations\n",
    "    recommendations = recipes_texts.sort_values(by='similarity', ascending=False).head(top_n)\n",
    "    return recommendations[['name', 'similarity']]\n",
    "\n",
    "# Example usage\n",
    "user_input = \"high protein low carb breakfast\"\n",
    "recommendations = recommend_recipes(user_input, recipes_texts, model)\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The code provided recommendations based on the similarity of the recipes to a given user input (\"high protein low carb breakfast\"). The top 5 recommended recipes are:\n",
    "\n",
    "    - Ham Cheddar and Jalapeno Crustless Quiche - Similarity: 0.668146\n",
    "    - Cream Cheese Biscuit - Similarity: 0.627648\n",
    "    - Vanilla Protein Pancakes - Similarity: 0.613065\n",
    "    - Banana Stuffed French Toast - Similarity: 0.611616\n",
    "    - Strawberry Banana Protein Smoothie - Similarity: 0.598489\n",
    "- Each recipe in the recommendation list has a similarity score, indicating how closely it matches the user’s input based on the embeddings generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of Each Step\n",
    "- Data Retrieval: We connect to the database, query the necessary fields, and load the data into a Pandas DataFrame.\n",
    "- Preprocessing: Combine descriptions, ingredients, and directions for each recipe to create a unified text field, which improves the quality of embeddings.\n",
    "- Generate Embeddings: Use sentence-transformers to encode the text data into numerical vectors. These embeddings allow for similarity calculations.\n",
    "Save Embeddings (Optional): Save embeddings back to the database, serialized as binary data. This is optional but useful for faster future access.\n",
    "- Recommendation Function: Calculate cosine similarity between the user’s input embedding and each recipe's embedding to find the most relevant recipes.\n",
    "Notes\n",
    "- Embeddings Storage: If you plan to use embeddings frequently, storing them back in the database can be efficient, though it requires serialization.\n",
    "- Model Choice: You can experiment with different Sentence Transformers models or use OpenAI's API if preferred (note that OpenAI usage incurs costs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings have been successfully stored in the database.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Database configuration\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'port': '5433',\n",
    "    'database': 'recipe_db',\n",
    "    'user': 'postgres',\n",
    "    'password': ''\n",
    "}\n",
    "\n",
    "# Establish connection using SQLAlchemy for easier database operations\n",
    "engine = create_engine(f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}\")\n",
    "\n",
    "# Step 1: Retrieve recipes and ingredients\n",
    "query = \"\"\"\n",
    "SELECT r.recipe_id, r.description, i.ingredient_text\n",
    "FROM recipes r\n",
    "JOIN ingredients i ON r.recipe_id = i.recipe_id;\n",
    "\"\"\"\n",
    "recipes_df = pd.read_sql(query, engine)\n",
    "\n",
    "# Combine description and ingredient_text for each recipe\n",
    "recipes_df['combined_text'] = recipes_df['description'] + \" \" + recipes_df['ingredient_text']\n",
    "\n",
    "# Group by recipe_id to aggregate all ingredient_text entries per recipe\n",
    "recipes_texts = recipes_df.groupby('recipe_id')['combined_text'].apply(' '.join).reset_index()\n",
    "\n",
    "# Step 2: Generate embeddings for each recipe\n",
    "recipes_texts['recipe_vector'] = recipes_texts['combined_text'].apply(lambda x: model.encode(x))\n",
    "\n",
    "# Step 3: Save the embeddings back to the database\n",
    "with engine.connect() as conn:\n",
    "    for idx, row in recipes_texts.iterrows():\n",
    "        # Convert the embedding to a format suitable for storage (e.g., as a string)\n",
    "        vector_str = np.array2string(row['recipe_vector'], separator=',')\n",
    "        conn.execute(\n",
    "            f\"\"\"\n",
    "            UPDATE recipes\n",
    "            SET recipe_vector = %s\n",
    "            WHERE recipe_id = %s\n",
    "            \"\"\",\n",
    "            (vector_str, row['recipe_id'])\n",
    "        )\n",
    "\n",
    "print(\"Embeddings have been successfully stored in the database.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
